# -*- coding: utf-8 -*-
"""Project 1  NLP Model for Classification Emotions in Text Using TensorFlow_Mario Christofell.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ynSRqZufaEcn3FnOZhpc9DFp0haDKXbL

# Mario Christofell L.Tobing

Project 1 Dicoding Modul Belajar Pengembangan Machine Learning 

*   NLP Model for Classification Emotions in Text
*   Dataset download from Kaggle : Emotions in text (https://www.kaggle.com/datasets/ishantjuyal/emotions-in-text)

# Import Library
"""

import pandas as pd
import nltk
import string
import os
import re
import tensorflow as tf
import matplotlib.pyplot as plt

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Embedding
from keras.models import Model
from keras.callbacks import EarlyStopping

nltk.download('wordnet')
nltk.download('stopwords')

"""# Import Dataset into Dataframe """

df = pd.read_csv('Emotion_final.csv') # Sumber dataset didownload dari Kaggle : Emotions in text (https://www.kaggle.com/datasets/ishantjuyal/emotions-in-text)
df.head()

df.info() # info dataset

"""# Data Cleansing"""

def cleansing(Text): # kolom 'content' yang akan dibersihkan
    
    # Menghapus huruf yang berulang
    cn1 = re.sub(r'(.)\1+', r'\1\1', Text)
    # Menghapus tanda baca yang berulang
    cn2 = re.sub(r'[\?\.\!]+(?=[\?.\!])', '', cn1)
    # Menghapus angka atau karakter (hanya alphabet saja yang diambil)
    cn3 = re.sub(r'[^a-zA-Z]', ' ', cn2)
    # Case Folding
    cn4 = cn3.lower()
    return cn4
for i, r in df.iterrows():
    y = cleansing(r['Text'])
    df.loc[i, 'Text'] = y

stopwrd = stopwords.words() # menghapus stopwords (kata umum/tidak ada arti)
def stopword(Text):
    return(' '.join([w for w in Text.split() if w not in stopwrd ]))
    df.Text = df.Text.apply(lambda x: stopword(x))

lemma = WordNetLemmatizer() # lematization
def lem(Text):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemma.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(Text.split())]))
    df.Text = df.Text.apply(lambda x: lem(x))

df.head()

"""# Model

Dataset yang dipakai merupakan dataset multiclass/categorical sehinggga perlu dilakukan prose One Hot Encoding
"""

# One Hot Encoding
category = pd.get_dummies(df.Emotion)
df_ohe = pd.concat([df, category], axis=1)
df_ohe = df_ohe.drop(columns='Emotion')
df_ohe.head(10)

"""Mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values untuk dapat diproses oleh model"""

txt = df_ohe['Text'].values
label = df_ohe[['anger', 'fear', 'happy', 'love', 'sadness', 'surprise']].values

"""Split data for data training and data testing/validation"""

txt_latih, txt_test, label_latih, label_test = train_test_split(txt, label, test_size=0.2) # pembagian dataset untuk data test/validation sebesar 20% dari total keseluruhan data

"""Mengubah setiap kata pada dataset ke dalam bentuk bilangan numerik menggunakan Tokenizer dan konversi menjadi sequence"""

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(txt_latih)
tokenizer.fit_on_texts(txt_test)

sekuens_latih = tokenizer.texts_to_sequences(txt_latih)
sekuens_test = tokenizer.texts_to_sequences(txt_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

"""Membangun arsitektur model dengan Sequential menggunakan layers LSTM dan Embedding lalu membuat fungsi compile dengan menentukan optimizer dan loss function yang dipakai oleh model  """

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(6, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

"""Membuat fungsi callback untuk memberi perintah berhenti melatih kepada model jika akurasi validasi sudah mencapai > 90% """

class myCallback(tf.keras.callbacks.Callback): #fungsi callback
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      self.model.stop_training = True
      print("\nThe val_accuracy has reached > 90%!")
callbacks = myCallback()

"""Mulai melatih model dengan fungsi fit"""

num_epochs = 50
history = model.fit(padded_latih, label_latih, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks], validation_steps=30)

"""# Visualization Plot Accuracy and Loss from Model"""

plt.plot(history.history['accuracy']) # plot accuracy
plt.plot(history.history['val_accuracy'])
plt.title('Plot Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

plt.plot(history.history['loss']) # plot loss
plt.plot(history.history['val_loss'])
plt.title('Plot Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()